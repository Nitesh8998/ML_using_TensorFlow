{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4 - Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nitesh8998/ML_using_TensorFlow/blob/master/Assignment_4_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLVhCarMwG70",
        "colab_type": "text"
      },
      "source": [
        "### **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXJ1XpSwdvR",
        "colab_type": "text"
      },
      "source": [
        "Install and import all the necessary libraries for the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQNMrFD-ZBwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "6fcae100-2d63-4a5c-cba8-c2fc994781ab"
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4b/77f0965ec7e8a76d3dcd6a22ca8bbd2b934cd92c4ded43fef6bea5ff3258/tensorflow-2.0.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.1.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.16.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.7.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.0.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.0.8)\n",
            "Collecting tb-nightly<1.15.0a20190807,>=1.15.0a20190806 (from tensorflow==2.0.0-rc0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/88/24b5fb7280e74c7cf65bde47c171547fd02afb3840cff41bcbe9270650f5/tb_nightly-1.15.0a20190806-py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 25.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.11.2)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 (from tensorflow==2.0.0-rc0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/28/f2a27a62943d5f041e4a6fd404b2d21cb7c59b2242a4e73b03d9ba166552/tf_estimator_nightly-1.14.0.dev2019080601-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-rc0) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0-rc0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (0.15.6)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed tb-nightly-1.15.0a20190806 tensorflow-2.0.0rc0 tf-estimator-nightly-1.14.0.dev2019080601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGgAUOKwsWA",
        "colab_type": "text"
      },
      "source": [
        "### **Importing the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOe2azQOdmND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston_dataset = load_boston()\n",
        "\n",
        "data_X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "data_Y = pd.DataFrame(boston_dataset.target, columns=[\"target\"])\n",
        "data = pd.concat([data_X, data_Y], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gD5esSxfxjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "e610097f-f6b8-4d1b-9d1d-4f40b215067b"
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "train, val = train_test_split(train, test_size=0.2, random_state=1)\n",
        "print(len(train), \"train examples\")\n",
        "print(len(val), \"validation examples\")\n",
        "print(len(test), \"test examples\")\n",
        "train.head()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "323 train examples\n",
            "81 validation examples\n",
            "102 test examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5240</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>13.91340</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7130</td>\n",
              "      <td>6.208</td>\n",
              "      <td>95.0</td>\n",
              "      <td>2.2222</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>100.63</td>\n",
              "      <td>15.17</td>\n",
              "      <td>11.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>0.07022</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>6.020</td>\n",
              "      <td>47.2</td>\n",
              "      <td>3.5549</td>\n",
              "      <td>5.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>16.6</td>\n",
              "      <td>393.23</td>\n",
              "      <td>10.11</td>\n",
              "      <td>23.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>0.23912</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5850</td>\n",
              "      <td>6.019</td>\n",
              "      <td>65.3</td>\n",
              "      <td>2.4091</td>\n",
              "      <td>6.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>12.92</td>\n",
              "      <td>21.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>0.21038</td>\n",
              "      <td>20.0</td>\n",
              "      <td>3.33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4429</td>\n",
              "      <td>6.812</td>\n",
              "      <td>32.2</td>\n",
              "      <td>4.1007</td>\n",
              "      <td>5.0</td>\n",
              "      <td>216.0</td>\n",
              "      <td>14.9</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.85</td>\n",
              "      <td>35.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         CRIM    ZN  INDUS  CHAS     NOX  ...    TAX  PTRATIO       B  LSTAT  target\n",
              "8     0.21124  12.5   7.87   0.0  0.5240  ...  311.0     15.2  386.63  29.93    16.5\n",
              "434  13.91340   0.0  18.10   0.0  0.7130  ...  666.0     20.2  100.63  15.17    11.7\n",
              "176   0.07022   0.0   4.05   0.0  0.5100  ...  296.0     16.6  393.23  10.11    23.2\n",
              "498   0.23912   0.0   9.69   0.0  0.5850  ...  391.0     19.2  396.90  12.92    21.2\n",
              "279   0.21038  20.0   3.33   0.0  0.4429  ...  216.0     14.9  396.90   4.85    35.1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZTeC55HxDeT",
        "colab_type": "text"
      },
      "source": [
        "Converting the Pandas DataFrames into Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF4GRPPLdTIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdZy7p3AaTRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "a43e1983-971a-4834-a518-fd2d3ce4d24e"
      },
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, True, batch_size)\n",
        "val_ds = df_to_dataset(val, False, batch_size)\n",
        "test_ds = df_to_dataset(test, False, batch_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63KuTr4sxMl6",
        "colab_type": "text"
      },
      "source": [
        "### Defining Feature Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "87427bab-065b-4cc3-9107-c4f4d6cf7dc6",
        "id": "YqYse_QCX614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# define feature_columns as a list of features using functions from tf.feature_column\n",
        "feature_columns = []\n",
        "for feature_batch, label_batch in train_ds.take(1):\n",
        "  A = list(feature_batch.keys())\n",
        "print(A)\n",
        "RAD = tf.feature_column.numeric_column(\"RAD\")\n",
        "rad_buckets = tf.feature_column.bucketized_column(RAD, boundaries=[2,5])\n",
        "for i in A:\n",
        "    feature_columns.append(tf.feature_column.numeric_column(i))\n",
        "\n",
        "feature_columns.append(rad_buckets)\n",
        "print(feature_columns)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
            "[NumericColumn(key='CRIM', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='ZN', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='INDUS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='CHAS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='NOX', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='RM', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='AGE', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='DIS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='RAD', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='TAX', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PTRATIO', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='B', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='LSTAT', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), BucketizedColumn(source_column=NumericColumn(key='RAD', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(2, 5))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVCMrdMxVB5",
        "colab_type": "text"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAc9LpVzqql9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6B9FgRyyGXe",
        "colab_type": "text"
      },
      "source": [
        "Model should contain following layers:\n",
        "\n",
        "```\n",
        "feature_layer\n",
        "\n",
        "Dense(1, activation=None)\n",
        "```\n",
        "\n",
        "Use 'Adam' optimizer\n",
        "\n",
        "Use 'mse' as your loss and metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZInuZ8D0xsu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build and compile your model in this cell.\n",
        "model = tf.keras.Sequential([\n",
        "    feature_layer,\n",
        "    #tf.keras.layers.Dense(4, activation=tf.nn.sigmoid),\n",
        "    tf.keras.layers.Dense(1, activation=None)\n",
        "])# Build and compile your model in this cell. Make sure to first answer Q2.\n",
        "\n",
        "ptimizer = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "model.compile(loss='mse',\n",
        "            optimizer='adam',\n",
        "            metrics=['mse'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igdzl3wasRo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fc5a72e5-8d6c-4b42-8d2d-20f265ac20d5"
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_15 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 62592.8089 - mse: 64102.8164 - val_loss: 0.0000e+00 - val_mse: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 59042.4648 - mse: 59139.6328 - val_loss: 53076.7161 - val_mse: 52354.6094\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 54393.3409 - mse: 54451.6602 - val_loss: 48822.6797 - val_mse: 48126.3906\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 50025.6850 - mse: 49975.6211 - val_loss: 44827.7695 - val_mse: 44157.7734\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 45696.6716 - mse: 45803.7852 - val_loss: 41134.0169 - val_mse: 40490.5352\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 41902.4280 - mse: 41962.9102 - val_loss: 37695.3242 - val_mse: 37079.3750\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 38354.3348 - mse: 38341.2500 - val_loss: 34507.1491 - val_mse: 33918.8789\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 35063.5367 - mse: 35032.5586 - val_loss: 31567.5293 - val_mse: 31006.6504\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 31925.6902 - mse: 31969.4219 - val_loss: 28844.7507 - val_mse: 28311.2539\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 29136.5593 - mse: 29136.3477 - val_loss: 26314.4941 - val_mse: 25808.5293\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 26549.4597 - mse: 26495.2578 - val_loss: 23983.2708 - val_mse: 23504.6426\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24099.5573 - mse: 24087.0020 - val_loss: 21807.0592 - val_mse: 21355.5508\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21831.1040 - mse: 21829.6289 - val_loss: 19814.8698 - val_mse: 19390.3516\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 19770.1343 - mse: 19786.8320 - val_loss: 17994.0625 - val_mse: 17596.3047\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17931.4932 - mse: 17914.6133 - val_loss: 16338.5726 - val_mse: 15967.0264\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16240.5938 - mse: 16215.6855 - val_loss: 14829.2985 - val_mse: 14483.4570\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14698.2157 - mse: 14680.9863 - val_loss: 13458.6488 - val_mse: 13137.9521\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13221.3862 - mse: 13262.6748 - val_loss: 12207.0140 - val_mse: 11910.9561\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12011.9423 - mse: 11982.8486 - val_loss: 11063.9740 - val_mse: 10792.1943\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10853.3534 - mse: 10842.7852 - val_loss: 10041.5202 - val_mse: 9793.0615\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9805.5391 - mse: 9802.4736 - val_loss: 9113.7039 - val_mse: 8887.9541\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8866.4215 - mse: 8859.7979 - val_loss: 8280.7274 - val_mse: 8076.8794\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8021.5765 - mse: 8017.6631 - val_loss: 7531.6833 - val_mse: 7348.9878\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7307.4105 - mse: 7268.2764 - val_loss: 6857.6545 - val_mse: 6695.3896\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6628.1961 - mse: 6606.6597 - val_loss: 6263.3146 - val_mse: 6120.4341\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5974.8942 - mse: 6003.1416 - val_loss: 5738.5719 - val_mse: 5613.9741\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5496.3887 - mse: 5483.3618 - val_loss: 5254.8141 - val_mse: 5148.2485\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4997.2862 - mse: 5005.2778 - val_loss: 4839.8228 - val_mse: 4749.7471\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4618.4945 - mse: 4602.3813 - val_loss: 4470.3659 - val_mse: 4395.8433\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4226.0619 - mse: 4235.4146 - val_loss: 4145.9006 - val_mse: 4085.9753\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3929.1906 - mse: 3916.2334 - val_loss: 3847.3025 - val_mse: 3801.7329\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3652.1630 - mse: 3629.5588 - val_loss: 3586.3645 - val_mse: 3554.2449\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3400.3618 - mse: 3385.2275 - val_loss: 3359.4272 - val_mse: 3339.7322\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3172.2044 - mse: 3166.7654 - val_loss: 3165.6396 - val_mse: 3156.9426\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2980.9960 - mse: 2983.3264 - val_loss: 2999.2301 - val_mse: 3000.1389\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2819.4035 - mse: 2818.0784 - val_loss: 2842.1476 - val_mse: 2852.8691\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2677.4864 - mse: 2667.9946 - val_loss: 2703.4980 - val_mse: 2723.1675\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2549.7909 - mse: 2541.4153 - val_loss: 2577.5474 - val_mse: 2605.8635\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2424.8147 - mse: 2421.5686 - val_loss: 2467.4687 - val_mse: 2503.5491\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2297.0056 - mse: 2317.9358 - val_loss: 2369.7844 - val_mse: 2412.7678\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2217.2766 - mse: 2231.2959 - val_loss: 2288.6004 - val_mse: 2336.6030\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2135.4746 - mse: 2151.0356 - val_loss: 2209.5051 - val_mse: 2262.9172\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2073.2072 - mse: 2078.9417 - val_loss: 2138.9862 - val_mse: 2197.0117\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2026.3609 - mse: 2017.4594 - val_loss: 2077.1818 - val_mse: 2138.7991\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1968.5561 - mse: 1959.3962 - val_loss: 2023.2840 - val_mse: 2088.0134\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1909.7764 - mse: 1907.9845 - val_loss: 1970.9837 - val_mse: 2039.1377\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1862.4585 - mse: 1858.2394 - val_loss: 1919.1499 - val_mse: 1990.8531\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1819.2712 - mse: 1812.8342 - val_loss: 1875.2933 - val_mse: 1949.3929\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1776.4924 - mse: 1771.4551 - val_loss: 1831.5724 - val_mse: 1908.3099\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1727.1268 - mse: 1730.2946 - val_loss: 1791.5376 - val_mse: 1870.3711\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1698.0609 - mse: 1693.8090 - val_loss: 1752.7154 - val_mse: 1833.5837\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1659.8856 - mse: 1657.6320 - val_loss: 1716.0898 - val_mse: 1798.8623\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1623.1779 - mse: 1623.7014 - val_loss: 1680.9246 - val_mse: 1765.4152\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1594.9704 - mse: 1591.4014 - val_loss: 1648.1510 - val_mse: 1734.0614\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1566.7157 - mse: 1561.4480 - val_loss: 1619.7479 - val_mse: 1705.2612\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1514.9564 - mse: 1533.1462 - val_loss: 1590.3472 - val_mse: 1676.1836\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1503.8344 - mse: 1504.9459 - val_loss: 1560.9937 - val_mse: 1645.5785\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1483.0221 - mse: 1475.8689 - val_loss: 1534.1611 - val_mse: 1617.5605\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1427.7896 - mse: 1449.2400 - val_loss: 1506.1150 - val_mse: 1589.4979\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1419.0391 - mse: 1420.3384 - val_loss: 1478.1708 - val_mse: 1560.7500\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1393.6720 - mse: 1394.0917 - val_loss: 1451.4926 - val_mse: 1533.3199\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1359.3719 - mse: 1367.5558 - val_loss: 1426.0639 - val_mse: 1507.4125\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1315.0538 - mse: 1342.4303 - val_loss: 1399.6368 - val_mse: 1480.3845\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1318.3364 - mse: 1315.4578 - val_loss: 1372.4583 - val_mse: 1451.2495\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1293.9961 - mse: 1288.8678 - val_loss: 1347.0769 - val_mse: 1425.1798\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1268.5939 - mse: 1265.1478 - val_loss: 1324.0730 - val_mse: 1400.6954\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1243.0100 - mse: 1241.2251 - val_loss: 1301.5846 - val_mse: 1377.0995\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1218.0092 - mse: 1218.8418 - val_loss: 1278.3189 - val_mse: 1353.0687\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1186.4759 - mse: 1195.3436 - val_loss: 1254.5271 - val_mse: 1329.0449\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1173.5216 - mse: 1172.2377 - val_loss: 1230.6917 - val_mse: 1304.8955\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1153.0298 - mse: 1149.9071 - val_loss: 1208.1518 - val_mse: 1282.1575\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1127.4145 - mse: 1128.2051 - val_loss: 1186.5396 - val_mse: 1259.3000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1113.4946 - mse: 1106.1342 - val_loss: 1165.1938 - val_mse: 1237.3339\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1078.8405 - mse: 1085.4508 - val_loss: 1144.2729 - val_mse: 1215.6831\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1061.0977 - mse: 1064.1848 - val_loss: 1122.8664 - val_mse: 1193.5536\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1049.9890 - mse: 1043.8184 - val_loss: 1102.2274 - val_mse: 1170.8801\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1023.7073 - mse: 1022.4888 - val_loss: 1082.7210 - val_mse: 1149.8265\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1010.7616 - mse: 1002.3439 - val_loss: 1063.4891 - val_mse: 1129.5747\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 974.8189 - mse: 983.5668 - val_loss: 1044.5317 - val_mse: 1109.2729\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 967.5772 - mse: 963.5594 - val_loss: 1025.1212 - val_mse: 1087.6547\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 944.2657 - mse: 943.5754 - val_loss: 1005.9485 - val_mse: 1067.3601\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 931.3682 - mse: 924.4385 - val_loss: 986.9899 - val_mse: 1047.0698\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 908.6233 - mse: 905.8625 - val_loss: 969.0773 - val_mse: 1027.9531\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 888.1060 - mse: 887.6068 - val_loss: 951.6780 - val_mse: 1008.9663\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 871.1351 - mse: 869.9664 - val_loss: 933.3942 - val_mse: 990.1213\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 857.7087 - mse: 852.3062 - val_loss: 915.3885 - val_mse: 971.4212\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 838.8659 - mse: 835.2256 - val_loss: 898.0755 - val_mse: 953.1600\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 815.1098 - mse: 818.0418 - val_loss: 881.3788 - val_mse: 935.2334\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 795.7260 - mse: 801.1011 - val_loss: 864.2838 - val_mse: 916.9325\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 774.5369 - mse: 783.2797 - val_loss: 846.4127 - val_mse: 897.7367\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 761.1396 - mse: 764.9261 - val_loss: 828.7453 - val_mse: 878.3196\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 751.6410 - mse: 747.6440 - val_loss: 811.6029 - val_mse: 859.3318\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 735.4439 - mse: 731.0709 - val_loss: 794.9581 - val_mse: 841.8273\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 710.6734 - mse: 715.2466 - val_loss: 779.3415 - val_mse: 825.3981\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 700.6689 - mse: 699.6462 - val_loss: 763.3434 - val_mse: 808.2102\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 688.9063 - mse: 684.6030 - val_loss: 748.2375 - val_mse: 791.9504\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 666.9147 - mse: 669.1862 - val_loss: 733.5096 - val_mse: 776.8100\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 653.5237 - mse: 655.3191 - val_loss: 717.9238 - val_mse: 760.2231\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 641.4374 - mse: 639.8107 - val_loss: 703.1723 - val_mse: 745.0682\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 629.6037 - mse: 626.9110 - val_loss: 688.9926 - val_mse: 729.7319\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 614.5540 - mse: 612.4703 - val_loss: 676.0374 - val_mse: 715.7535\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 599.2314 - mse: 599.6351 - val_loss: 662.1921 - val_mse: 701.5815\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 589.2116 - mse: 585.9965 - val_loss: 648.9038 - val_mse: 687.4405\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 569.4919 - mse: 573.3608 - val_loss: 635.9431 - val_mse: 673.7537\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 557.1272 - mse: 560.5085 - val_loss: 622.9415 - val_mse: 660.1554\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 550.0491 - mse: 548.0461 - val_loss: 609.4230 - val_mse: 645.4875\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 536.9231 - mse: 535.3036 - val_loss: 597.1075 - val_mse: 632.3041\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 522.6013 - mse: 523.4543 - val_loss: 584.8316 - val_mse: 619.4723\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 514.2654 - mse: 511.7892 - val_loss: 573.0290 - val_mse: 607.1124\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 503.3445 - mse: 500.4052 - val_loss: 561.8858 - val_mse: 594.8158\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 492.3547 - mse: 489.7058 - val_loss: 550.8978 - val_mse: 582.9164\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 482.5246 - mse: 478.9760 - val_loss: 540.1636 - val_mse: 571.5760\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 471.3126 - mse: 468.9478 - val_loss: 529.5231 - val_mse: 560.5867\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 460.9761 - mse: 459.0714 - val_loss: 518.9627 - val_mse: 549.5801\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 451.4061 - mse: 448.8487 - val_loss: 508.8411 - val_mse: 538.9845\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 443.3117 - mse: 439.7476 - val_loss: 498.8034 - val_mse: 528.6838\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 429.1455 - mse: 430.3281 - val_loss: 489.2873 - val_mse: 518.5476\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 420.4307 - mse: 421.2310 - val_loss: 479.7072 - val_mse: 508.0142\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 414.3684 - mse: 411.8470 - val_loss: 470.4371 - val_mse: 498.0211\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 399.2912 - mse: 403.0418 - val_loss: 461.6169 - val_mse: 488.5743\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 396.2972 - mse: 394.5980 - val_loss: 452.1794 - val_mse: 478.3811\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 383.9353 - mse: 385.9940 - val_loss: 443.4243 - val_mse: 468.9887\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 371.6835 - mse: 377.5734 - val_loss: 433.8042 - val_mse: 458.7690\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 365.9673 - mse: 368.4159 - val_loss: 424.4846 - val_mse: 449.3363\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 360.5592 - mse: 360.3470 - val_loss: 415.5409 - val_mse: 440.3419\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 350.1092 - mse: 352.3239 - val_loss: 407.1611 - val_mse: 431.6792\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 341.7297 - mse: 344.1875 - val_loss: 398.6459 - val_mse: 422.2792\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 338.2685 - mse: 336.4539 - val_loss: 390.7711 - val_mse: 414.0875\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 331.0962 - mse: 329.3186 - val_loss: 383.2391 - val_mse: 405.8361\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 323.9397 - mse: 322.4848 - val_loss: 375.9457 - val_mse: 397.8462\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 316.5287 - mse: 315.7318 - val_loss: 368.9489 - val_mse: 390.3732\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 309.5080 - mse: 309.4370 - val_loss: 362.5234 - val_mse: 383.6808\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 305.2492 - mse: 303.4398 - val_loss: 355.9279 - val_mse: 376.7008\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 299.2179 - mse: 297.4550 - val_loss: 349.5909 - val_mse: 369.8094\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 290.6830 - mse: 291.7723 - val_loss: 343.2797 - val_mse: 362.9196\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 287.9561 - mse: 285.8167 - val_loss: 337.0334 - val_mse: 356.3444\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 282.1362 - mse: 280.2778 - val_loss: 331.2493 - val_mse: 350.2493\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 275.6750 - mse: 275.0497 - val_loss: 325.5922 - val_mse: 344.1391\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 270.5851 - mse: 269.8964 - val_loss: 319.4840 - val_mse: 337.4821\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 263.0759 - mse: 264.5969 - val_loss: 313.7786 - val_mse: 331.2354\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 257.5761 - mse: 259.3263 - val_loss: 308.0986 - val_mse: 325.3371\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 255.5804 - mse: 254.0435 - val_loss: 302.1765 - val_mse: 319.1136\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 249.4228 - mse: 249.0968 - val_loss: 296.7878 - val_mse: 313.4334\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 245.6609 - mse: 244.2567 - val_loss: 291.4064 - val_mse: 307.7624\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 239.6918 - mse: 239.6937 - val_loss: 286.5248 - val_mse: 302.6380\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 234.7705 - mse: 235.7111 - val_loss: 282.0083 - val_mse: 298.0909\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 229.8663 - mse: 231.5054 - val_loss: 277.3339 - val_mse: 293.1382\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 229.1491 - mse: 227.2940 - val_loss: 272.5505 - val_mse: 288.1425\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 223.5116 - mse: 223.2560 - val_loss: 268.2211 - val_mse: 283.5374\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 218.2105 - mse: 219.5143 - val_loss: 263.9974 - val_mse: 279.2983\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 217.2778 - mse: 215.7833 - val_loss: 259.6606 - val_mse: 274.8238\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 211.9052 - mse: 212.1758 - val_loss: 255.6102 - val_mse: 270.5438\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 202.3663 - mse: 208.5631 - val_loss: 251.3732 - val_mse: 265.9915\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 199.2104 - mse: 204.6925 - val_loss: 247.4302 - val_mse: 262.0221\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 200.1014 - mse: 201.3332 - val_loss: 243.5140 - val_mse: 258.1551\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 196.9418 - mse: 198.7710 - val_loss: 240.4010 - val_mse: 255.0706\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 194.5265 - mse: 195.3719 - val_loss: 236.5961 - val_mse: 250.6651\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 192.3532 - mse: 192.3440 - val_loss: 232.8074 - val_mse: 246.4567\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 188.2729 - mse: 188.8972 - val_loss: 228.9583 - val_mse: 242.1216\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 182.9679 - mse: 185.8860 - val_loss: 225.4159 - val_mse: 238.1656\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 183.2753 - mse: 183.1255 - val_loss: 222.0025 - val_mse: 234.5954\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 181.1917 - mse: 180.4129 - val_loss: 218.8816 - val_mse: 231.2937\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 179.4034 - mse: 177.9650 - val_loss: 215.7836 - val_mse: 227.8462\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 176.7385 - mse: 175.4394 - val_loss: 213.0876 - val_mse: 224.9727\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 173.4346 - mse: 173.2507 - val_loss: 210.4690 - val_mse: 222.2306\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 168.5177 - mse: 171.0573 - val_loss: 207.6508 - val_mse: 219.2113\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 169.9459 - mse: 168.7480 - val_loss: 204.7779 - val_mse: 216.1119\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 167.5894 - mse: 166.5818 - val_loss: 202.3201 - val_mse: 213.5343\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 165.4660 - mse: 164.7177 - val_loss: 199.7837 - val_mse: 210.7838\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 163.7246 - mse: 162.6917 - val_loss: 197.4479 - val_mse: 208.3780\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 161.2491 - mse: 160.8084 - val_loss: 195.2601 - val_mse: 206.0980\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 158.6857 - mse: 159.1329 - val_loss: 192.9835 - val_mse: 203.6238\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 156.8800 - mse: 157.4294 - val_loss: 190.7557 - val_mse: 201.3496\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 152.4037 - mse: 155.6240 - val_loss: 188.6560 - val_mse: 199.2455\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 155.1156 - mse: 154.0047 - val_loss: 186.7562 - val_mse: 197.3828\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 153.6679 - mse: 152.4488 - val_loss: 184.9588 - val_mse: 195.5434\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.8842 - mse: 151.0555 - val_loss: 183.1648 - val_mse: 193.6639\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 150.5004 - mse: 149.7399 - val_loss: 181.0912 - val_mse: 191.3583\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 148.6635 - mse: 148.1612 - val_loss: 179.5071 - val_mse: 189.7489\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 147.5370 - mse: 146.8600 - val_loss: 177.6078 - val_mse: 187.7217\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 146.7537 - mse: 145.5523 - val_loss: 176.0500 - val_mse: 186.0618\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 145.5071 - mse: 144.4548 - val_loss: 174.4585 - val_mse: 184.3522\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 143.2944 - mse: 143.2323 - val_loss: 172.8822 - val_mse: 182.6719\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 140.5581 - mse: 142.0180 - val_loss: 171.2302 - val_mse: 180.8605\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 138.4584 - mse: 140.8867 - val_loss: 169.6033 - val_mse: 179.1644\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 139.7478 - mse: 139.6813 - val_loss: 167.8897 - val_mse: 177.3806\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 139.1284 - mse: 138.5584 - val_loss: 166.4143 - val_mse: 175.9320\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 136.7851 - mse: 137.3632 - val_loss: 164.9662 - val_mse: 174.4212\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 133.4176 - mse: 136.2984 - val_loss: 163.5924 - val_mse: 173.0182\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 134.8813 - mse: 135.2230 - val_loss: 162.0638 - val_mse: 171.4220\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 135.3050 - mse: 134.2757 - val_loss: 160.9966 - val_mse: 170.3163\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 133.7402 - mse: 133.5002 - val_loss: 159.8135 - val_mse: 169.0367\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 130.0731 - mse: 132.7036 - val_loss: 158.5164 - val_mse: 167.6367\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 132.1665 - mse: 132.0193 - val_loss: 157.5315 - val_mse: 166.7297\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 128.9251 - mse: 131.0132 - val_loss: 156.2114 - val_mse: 165.3223\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 131.0410 - mse: 130.1045 - val_loss: 155.1136 - val_mse: 164.2610\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 130.3190 - mse: 129.3349 - val_loss: 154.0377 - val_mse: 163.1335\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 128.3221 - mse: 128.6172 - val_loss: 152.9061 - val_mse: 161.8938\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 127.3818 - mse: 128.0034 - val_loss: 152.0179 - val_mse: 160.9995\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 122.2621 - mse: 127.6484 - val_loss: 150.6775 - val_mse: 159.4552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f876622d2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFu2k4J_spfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}